{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+storm surge&publicationDateOrYear=2017&fields=title,url,abstract,year,authors,citationCount\n",
      "214\n",
      "3\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+hurricane&publicationDateOrYear=2017&fields=title,url,abstract,year,authors,citationCount\n",
      "58\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+storm surge&publicationDateOrYear=2018&fields=title,url,abstract,year,authors,citationCount\n",
      "266\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+hurricane&publicationDateOrYear=2018&fields=title,url,abstract,year,authors,citationCount\n",
      "59\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+storm surge&publicationDateOrYear=2019&fields=title,url,abstract,year,authors,citationCount\n",
      "276\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+hurricane&publicationDateOrYear=2019&fields=title,url,abstract,year,authors,citationCount\n",
      "73\n",
      "0\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+storm surge&publicationDateOrYear=2020&fields=title,url,abstract,year,authors,citationCount\n",
      "360\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+hurricane&publicationDateOrYear=2020&fields=title,url,abstract,year,authors,citationCount\n",
      "97\n",
      "0\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+storm surge&publicationDateOrYear=2021&fields=title,url,abstract,year,authors,citationCount\n",
      "329\n",
      "2\n",
      "6\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+hurricane&publicationDateOrYear=2021&fields=title,url,abstract,year,authors,citationCount\n",
      "80\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+storm surge&publicationDateOrYear=2022&fields=title,url,abstract,year,authors,citationCount\n",
      "232\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+hurricane&publicationDateOrYear=2022&fields=title,url,abstract,year,authors,citationCount\n",
      "69\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+storm surge&publicationDateOrYear=2023&fields=title,url,abstract,year,authors,citationCount\n",
      "307\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+hurricane&publicationDateOrYear=2023&fields=title,url,abstract,year,authors,citationCount\n",
      "84\n",
      "0\n",
      "1\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+storm surge&publicationDateOrYear=2024&fields=title,url,abstract,year,authors,citationCount\n",
      "124\n",
      "4\n",
      "https://api.semanticscholar.org/graph/v1/paper/search?query=water level+satellite+predict+hurricane&publicationDateOrYear=2024&fields=title,url,abstract,year,authors,citationCount\n",
      "24\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# country_list = [\n",
    "#     'Antigua and Barbuda', 'Bahamas', 'Barbados', 'Belize', 'Canada',\n",
    "#     'Costa Rica', 'Cuba', 'Dominica', 'Dominican Republic', 'El Salvador',\n",
    "#     'Grenada', 'Guatemala', 'Haiti', 'Honduras', 'Jamaica', 'Mexico',\n",
    "#     'Nicaragua', 'Panama', 'Saint Kitts and Nevis', 'Saint Lucia',\n",
    "#     'Saint Vincent and the Grenadines', 'Trinidad and Tobago', 'United States'\n",
    "# ]\n",
    "\n",
    "country_list = ['storm surge', 'hurricane']\n",
    "api_key = 'XBbGPcpPXB5CH0aLSu6BO5fww06zAusz6dFGXqj0'\n",
    "\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'x-api-key': api_key\n",
    "}\n",
    "\n",
    "max_limit = 100  # The maximum limit per request\n",
    "max_offset = 1000  # The maximum offset\n",
    "\n",
    "for this_year in range(2017, 2025):\n",
    "    for this_country in country_list:\n",
    "        paper_id_list = []\n",
    "        url_list = []\n",
    "        title_list = []\n",
    "        abstract_list = []\n",
    "        authors_list = []\n",
    "        year_list = []\n",
    "        citationCount_list = []\n",
    "        keywords = ['water level', 'satellite', 'predict', this_country]\n",
    "\n",
    "        query = '+'.join(keywords)\n",
    "        this_date = str(this_year)+'-'+f\"{this_month:02}\"\n",
    "        results_url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={query}&publicationDateOrYear={this_year}&fields=title,url,abstract,year,authors,citationCount\"\n",
    "        print(results_url)\n",
    "        # Fetch the total number of results\n",
    "        response = requests.get(results_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch data for {query} in {this_year}. Status code: {response.status_code}\")\n",
    "            continue\n",
    "        data = response.json()\n",
    "        print(data.get('total', 0))\n",
    "\n",
    "        if data.get('total', 0) > 1000:\n",
    "\n",
    "            for this_month in range(1, 13):\n",
    "                    \n",
    "                keywords = ['water level', 'satellite', 'time series','computer vision', this_country]\n",
    "                query = '+'.join(keywords)\n",
    "                this_date = str(this_year)+'-'+f\"{this_month:02}\"\n",
    "                results_url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={query}&publicationDateOrYear={this_date}&fields=title,url,abstract,year,authors,citationCount\"\n",
    "                print(results_url)\n",
    "                # Fetch the total number of results\n",
    "                response = requests.get(results_url, headers=headers)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Failed to fetch data for {query} in {this_year}. Status code: {response.status_code}\")\n",
    "                    continue\n",
    "                data = response.json()\n",
    "                print(data.get('total', 0))\n",
    "                total_number = min(data.get('total', 0), max_offset)  # Ensure we don't exceed 1000\n",
    "                print(total_number)\n",
    "                count_offset = 0\n",
    "                total_data = []\n",
    "                while count_offset < total_number:\n",
    "                    limit = min(max_limit, total_number - count_offset)  # Adjust limit if nearing total_number\n",
    "                    this_results_url = f\"{results_url}&offset={count_offset}&limit={limit}\"\n",
    "                    response = requests.get(this_results_url, headers=headers)\n",
    "                    if response.status_code != 200:\n",
    "                        print(f\"Failed to fetch data for offset {count_offset}. Status code: {response.status_code}\")\n",
    "                        break\n",
    "\n",
    "                    data = response.json()\n",
    "                    papers = data.get('data', [])\n",
    "                    if not papers:\n",
    "                        print(\"No data found at this offset.\")\n",
    "                        break\n",
    "\n",
    "                    total_data.extend(papers)\n",
    "                    count_offset += limit\n",
    "                    time.sleep(1)  # To respect API rate limits\n",
    "\n",
    "                for this_paper_id in range(len(total_data)):\n",
    "                            this_paper = total_data[this_paper_id]\n",
    "                            abstract = this_paper.get('abstract')\n",
    "                            if abstract and ('satellite' in abstract):\n",
    "                                print(this_paper_id)\n",
    "                                paper_id_list.append(this_paper.get('paperId', 'N/A'))\n",
    "                                url_list.append(this_paper.get('url', 'N/A'))\n",
    "                                title_list.append(this_paper.get('title', 'N/A'))\n",
    "                                abstract_list.append(this_paper.get('abstract', 'N/A'))\n",
    "                                authors_list.append(', '.join([author['name'] for author in this_paper.get('authors', [])]))\n",
    "                                year_list.append(this_paper.get('year', 'N/A'))\n",
    "                                citationCount_list.append(this_paper.get('citationCount', 0))\n",
    "        else:\n",
    "            count_offset = 0\n",
    "            total_data = []\n",
    "            while count_offset < total_number:\n",
    "                limit = min(max_limit, total_number - count_offset)  # Adjust limit if nearing total_number\n",
    "                this_results_url = f\"{results_url}&offset={count_offset}&limit={limit}\"\n",
    "                response = requests.get(this_results_url, headers=headers)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Failed to fetch data for offset {count_offset}. Status code: {response.status_code}\")\n",
    "                    break\n",
    "\n",
    "                data = response.json()\n",
    "                papers = data.get('data', [])\n",
    "                if not papers:\n",
    "                    print(\"No data found at this offset.\")\n",
    "                    break\n",
    "\n",
    "                total_data.extend(papers)\n",
    "                count_offset += limit\n",
    "                time.sleep(1)  # To respect API rate limits\n",
    "\n",
    "            for this_paper_id in range(len(total_data)):\n",
    "                        this_paper = total_data[this_paper_id]\n",
    "                        abstract = this_paper.get('abstract')\n",
    "                        if abstract and ('satellite' in abstract):\n",
    "                            print(this_paper_id)\n",
    "                            paper_id_list.append(this_paper.get('paperId', 'N/A'))\n",
    "                            url_list.append(this_paper.get('url', 'N/A'))\n",
    "                            title_list.append(this_paper.get('title', 'N/A'))\n",
    "                            abstract_list.append(this_paper.get('abstract', 'N/A'))\n",
    "                            authors_list.append(', '.join([author['name'] for author in this_paper.get('authors', [])]))\n",
    "                            year_list.append(this_paper.get('year', 'N/A'))\n",
    "                            citationCount_list.append(this_paper.get('citationCount', 0))\n",
    "        if len(title_list) == 0:\n",
    "            continue\n",
    "        paper_repos_dict = {\n",
    "            'title': title_list,\n",
    "            'abstract': abstract_list,\n",
    "            'year': year_list,\n",
    "            'citationCount': citationCount_list,\n",
    "            'url': url_list,\n",
    "            'paperId': paper_id_list,\n",
    "            'authors': authors_list\n",
    "        }\n",
    "\n",
    "        this_paper_sum = pd.DataFrame(paper_repos_dict)\n",
    "        this_paper_sum.to_csv(f'/Users/liting/Documents/GitHub/google-scholar-search/storm_surge_search/{\"_\".join(keywords)}_{this_year}.csv', sep=',', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_all_csv_files_in_directory(directory_path):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(directory_path)\n",
    "    # Filter out only CSV files\n",
    "    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "    \n",
    "    # List to hold dataframes\n",
    "    dataframes = []\n",
    "    \n",
    "    # Iterate over CSV files and read them into a list of dataframes\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(directory_path, csv_file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    # Optionally, concatenate all dataframes into a single dataframe\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    return combined_df, dataframes\n",
    "\n",
    "# Example usage\n",
    "directory_path = '/Users/liting/Documents/GitHub/google-scholar-search/storm_search'\n",
    "combined_df, dataframes = read_all_csv_files_in_directory(directory_path)\n",
    "combined_df=combined_df.drop_duplicates()\n",
    "combined_df.to_csv(f'/Users/liting/Documents/GitHub/google-scholar-search/storm_surge_search/combined_df.csv', sep=',', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
