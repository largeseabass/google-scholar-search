{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from scholarly import scholarly\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following the guidance of: https://medium.com/@nandinisaini021/scraping-publications-of-aerial-image-research-papers-on-google-scholar-using-python-a0dee9744728\n",
    "LH: fix the issue of getting citations\n",
    "TODO: getting abstract. It seems impossible to get it from google scholar??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function for the getting inforamtion of the web page\n",
    "def get_paperinfo(paper_url):\n",
    "\n",
    "  #download the page\n",
    "  response=requests.get(url,headers=headers)\n",
    "\n",
    "  # check successful response\n",
    "  if response.status_code != 200:\n",
    "    print('Status code:', response.status_code)\n",
    "    raise Exception('Failed to fetch web page ')\n",
    "\n",
    "  #parse using beautiful soup\n",
    "  paper_doc = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "  return paper_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(doc):\n",
    "  # select finds multiple instances and returns a list, find finds the first\n",
    "  paper_tag = doc.select('[data-lid]')\n",
    "  #cite_tag = doc.select('[title=Cite] + a')\n",
    "  cite_tag0 = doc.find_all(\"a\",class_='gs_or_cit gs_or_btn gs_nph')\n",
    "  cite_tag = cite_tag0.copy()\n",
    "  for i in range(len(cite_tag0)):\n",
    "    full_text = cite_tag0[i]\n",
    "    this_two = full_text.find_next(\"a\")\n",
    "    cite_tag[i]= this_two\n",
    "  \n",
    "\n",
    "  link_tag = doc.find_all('h3',{\"class\" : \"gs_rt\"})\n",
    "  author_tag = doc.find_all(\"div\", {\"class\": \"gs_a\"})\n",
    "\n",
    "  return paper_tag,cite_tag,link_tag,author_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will return the title of the paper\n",
    "def get_papertitle(paper_tag):\n",
    "  \n",
    "  paper_names = []\n",
    "  \n",
    "  for tag in paper_tag:\n",
    "    paper_names.append(tag.select('h3')[0].get_text())\n",
    "\n",
    "  return paper_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will return the number of citation of the paper\n",
    "def get_citecount(cite_tag):\n",
    "  cite_count = []\n",
    "  for i in cite_tag:\n",
    "    cite = i.text\n",
    "    if i is None or cite is None:  # if paper has no citatation then consider 0\n",
    "      cite_count.append(0)\n",
    "    else:\n",
    "      tmp = re.search(r'\\d+', cite) # its handle the None type object error and re use to remove the string \" cited by \" and return only integer value\n",
    "      if tmp is None :\n",
    "        cite_count.append(0)\n",
    "      else :\n",
    "        cite_count.append(int(tmp.group()))\n",
    "\n",
    "  return cite_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for the getting link information\n",
    "def get_link(link_tag):\n",
    "\n",
    "  links = []\n",
    "\n",
    "  for i in range(len(link_tag)) :\n",
    "    links.append(link_tag[i].a['href']) \n",
    "\n",
    "  return links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for the getting autho , year and publication information\n",
    "def get_author_year_publi_info(authors_tag):\n",
    "  years = []\n",
    "  publication = []\n",
    "  authors = []\n",
    "  for i in range(len(authors_tag)):\n",
    "      authortag_text = (authors_tag[i].text).split()\n",
    "      try: #LH\n",
    "        year = int(re.search(r'\\d+', authors_tag[i].text).group())\n",
    "      except:\n",
    "        year = np.nan\n",
    "      years.append(year)\n",
    "      publication.append(authortag_text[-1])\n",
    "      author = authortag_text[0] + ' ' + re.sub(',','', authortag_text[1])\n",
    "      authors.append(author)\n",
    "  \n",
    "  return years , publication, authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding information in repository\n",
    "def add_in_paper_repo(papername,year,author,cite,publi,link):\n",
    "  paper_repos_dict['Paper Title'].extend(papername)\n",
    "  paper_repos_dict['Year'].extend(year)\n",
    "  paper_repos_dict['Author'].extend(author)\n",
    "  paper_repos_dict['Citation'].extend(cite)\n",
    "  paper_repos_dict['Publication'].extend(publi)\n",
    "  paper_repos_dict['Url of paper'].extend(link)\n",
    "\n",
    "  return pd.DataFrame(paper_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "Status code: 429\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to fetch web page ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://scholar.google.com/scholar?start=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m&q=Coastal+Surge,+model,++texas&hl=en&as_sdt=0,44\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i)\n\u001b[1;32m     18\u001b[0m \u001b[39m# function for the get content of each page\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m doc \u001b[39m=\u001b[39m get_paperinfo(url)\n\u001b[1;32m     21\u001b[0m \u001b[39m# function for the collecting tags\u001b[39;00m\n\u001b[1;32m     22\u001b[0m paper_tag,cite_tag,link_tag,author_tag \u001b[39m=\u001b[39m get_tags(doc)\n",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m, in \u001b[0;36mget_paperinfo\u001b[0;34m(paper_url)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m      9\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStatus code:\u001b[39m\u001b[39m'\u001b[39m, response\u001b[39m.\u001b[39mstatus_code)\n\u001b[0;32m---> 10\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFailed to fetch web page \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39m#parse using beautiful soup\u001b[39;00m\n\u001b[1;32m     13\u001b[0m paper_doc \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext,\u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Failed to fetch web page "
     ]
    }
   ],
   "source": [
    "# creating final repository\n",
    "paper_repos_dict = {\n",
    "                    'Paper Title' : [],\n",
    "                    'Year' : [],\n",
    "                    'Author' : [],\n",
    "                    'Citation' : [],\n",
    "                    'Publication' : [],\n",
    "                    'Url of paper' : [] }\n",
    "for i in range (0,490,10):\n",
    "  print(i)\n",
    "\n",
    "\n",
    "  # get url for the each page\n",
    "  # open google scholar, search with keywords, click: next page, copy and paste the url below\n",
    "  # delete the numbers after start=, and add .format(i) at the end.\n",
    "  #url = 'https://scholar.google.com/scholar?start={}&q=Burkholderia+Pseudomallei&hl=en&as_sdt=0,5'.format(i)\n",
    "  url = 'https://scholar.google.com/scholar?start={}&q=Coastal+Surge,+model,++texas&hl=en&as_sdt=0,44'.format(i)\n",
    "  # function for the get content of each page\n",
    "  doc = get_paperinfo(url)\n",
    "\n",
    "  # function for the collecting tags\n",
    "  paper_tag,cite_tag,link_tag,author_tag = get_tags(doc)\n",
    "\n",
    "  # paper title from each page\n",
    "  papername = get_papertitle(paper_tag)\n",
    "  \n",
    "  # year , author , publication of the paper\n",
    "  year , publication , author = get_author_year_publi_info(author_tag)\n",
    "\n",
    "  # cite count of the paper \n",
    "  cite = get_citecount(cite_tag)\n",
    "\n",
    "  # url of the paper\n",
    "  link = get_link(link_tag)\n",
    "\n",
    "  # add in paper repo dict\n",
    "  final = add_in_paper_repo(papername,year,author,cite,publication,link)\n",
    "  \n",
    "  # use sleep to avoid status code 429\n",
    "  time.sleep(30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to your path\n",
    "final.to_csv('/Users/vivianhuang/Documents/GitHub/google-scholar-search/csv_results/coastal_surge_model_texas_500.csv', sep=',', index=False,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
